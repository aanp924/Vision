{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f582cf24-9cb3-47a5-bc42-f4ac4db8eb34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Download and train yolo11n on coco8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3c9a1-222e-47cb-a808-7fae9edc52ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "    # Load a pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "    # Train the model on the COCO8 dataset for 100 epochs\n",
    "train_results = model.train(\n",
    "    data=\"coco8.yaml\",  # Path to dataset configuration file\n",
    "    epochs=100,  # Number of training epochs\n",
    "    imgsz=640,  # Image size for training\n",
    ")\n",
    "\n",
    "    # Evaluate the model's performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "    # Perform object detection on an image\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")  # Predict on an image\n",
    "results[0].show()  # Display results\n",
    "\n",
    "    # Export the model to ONNX format for deployment\n",
    "#path = model.export(format=\"onnx\")  # Returns the path to the exported model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9939c8-ca54-4d9b-b81b-dc6dbd1f0c6d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Train on 57 annotated image sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618fa52-fc10-43d8-92f0-cbd918c02972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train the model on the COCO8 dataset for 100 epochs\n",
    "train_results = model.train( data=\"./datasets/10_03_25_57imgs/data.yaml\", # Path to dataset configuration file \n",
    "                             epochs=100, # Number of training epochs \n",
    "                             imgsz=640, # Image size for training \n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5076af4-5642-4669-8adf-166184513e5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Train on 137 augmented/annotated image sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723122b2-015f-4750-b429-94d91620aede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train the model on the COCO8 dataset for 100 epochs\n",
    "train_results = model.train( data=\"./datasets/10_03_25__137imgs_augmented//data.yaml\", # Path to dataset configuration file \n",
    "                             epochs=100, # Number of training epochs \n",
    "                             imgsz=640, # Image size for training \n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0d09d-fdd9-41fe-9953-c64c454ffd3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Inference on videostream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660760ab-0c97-4b87-a82d-3b54cc57a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import time\n",
    "\t\n",
    "# Load a pretrained YOLO11n model\n",
    "model = YOLO(\"./weights/best.pt\")\n",
    "\n",
    "inference = model(\"./datasets/10_03_25_57imgs/test/images/IMG_0432_JPG.rf.051c3c92a77f62b6609178e5f7d85fbb.jpg\")\n",
    "result = inference[0]\n",
    "detections = sv.Detections.from_ultralytics (result)\n",
    "detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7c657-9f6e-456f-acbb-4006763bda57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Path Detection - Inference/Bounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238a5d99-7d86-41b8-8a38-2815ce9de4cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vid_cap = cv2.VideoCapture('./datasets/recording/IMG_5795.MOV')\n",
    "while (vid_cap.isOpened()):\n",
    "    _, frame = vid_cap.read()\n",
    "    inference = model( frame, )[0]\n",
    "\n",
    "    im_bgr = inference.plot()  # BGR-order numpy array\n",
    "    #im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "\n",
    "    cv2.imshow('result', cv2.resize(im_bgr, (640, 640)) )\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37476dde-122d-4a01-8400-979d6f5acfa4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Centroids On Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffad62-0d27-4fa9-bf8f-972794ed2e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "radius = 5                    # Radius in pixels\n",
    "color = (255, 0, 255)             # BGR color (Green in this case)\n",
    "thickness = 2                   # Thickness of the circle line (-1 for a filled circle)\n",
    "thickness = -1 # -1 indicates a filled circle   \n",
    "\n",
    "\n",
    "vid_cap = cv2.VideoCapture('./datasets/recording/IMG_5795.MOV')\n",
    "while (vid_cap.isOpened()):\n",
    "    _, frame = vid_cap.read()\n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    inference = model(frame, verbose=False)[0]\n",
    "   \n",
    "    detections = sv.Detections.from_ultralytics( inference)\n",
    "    walkway = detections[detections.class_id == 4] #walkway\n",
    "    if len(walkway) < 3:\n",
    "        continue\n",
    "    \n",
    "    centroid_list = [((b1[0] + b1[2])/2, (b1[1] + b1[3])/2) for b1 in walkway.xyxy ]\n",
    "    centroid_x = [ int(x[0]) for x in centroid_list]\n",
    "    centroid_y = [ int(y[1]) for y in centroid_list]\n",
    "    \n",
    "    for i, x in enumerate(centroid_x):\n",
    "        #Define circle parameters\n",
    "        center_coordinates = (x, centroid_y[i])  # (x, y) - center of the circle\n",
    "        # 3. Draw the circle on the image\n",
    "        cv2.circle(frame, center_coordinates, radius, color, thickness)\n",
    "    \n",
    "    # 4. Display the image with the centroids\n",
    "    cv2.imshow('result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "vid_cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf810f0-a012-456b-b933-3c5216223cb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Mean of Centroids/Frame On Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5d49eb-8e06-44ad-8e6f-65c3e7c437c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 14.42s for 290 frames\n",
      "frames 20.110957004160888 per seconds\n"
     ]
    }
   ],
   "source": [
    "data_centroid_x = np.empty([0])\n",
    "data_centroid_y = np.empty([0])\n",
    "\n",
    "data_centroid_list_x = np.empty([0])\n",
    "data_centroid_list_y = np.empty([0])\n",
    "data_walkways = np.empty([0])\n",
    "\n",
    "model = YOLO(\"./weights/best.pt\")\n",
    "vid_cap = cv2.VideoCapture('./datasets/recording/IMG_5795.MOV')\n",
    "total_frames = 0\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "while (vid_cap.isOpened()):\n",
    "   \n",
    "    # Read a frame from the video\n",
    "    ret, frame = vid_cap.read()\n",
    "\n",
    "    # If ret is False, it means the video has ended\n",
    "    if not ret:\n",
    "        print(\"Video has finished.\")\n",
    "        break\n",
    "        \n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    inference = model(frame, verbose=False)[0]\n",
    "   \n",
    "    walkway = sv.Detections.from_ultralytics( inference)\n",
    "    #walkway = detections[(detections.class_id == 4) & (detections.confidence > 0.5)] #walkway\n",
    "    \n",
    "    total_frames += 1\n",
    "    \n",
    "    if len(walkway) < 3:\n",
    "        continue      \n",
    "    \n",
    "    data_walkways = np.append(data_walkways, len(walkway))\n",
    "    \n",
    "    centroid_list = [((b1[0] + b1[2])/2, (b1[1] + b1[3])/2) for b1 in walkway.xyxy ]\n",
    "    centroid_list.sort(key=lambda p: p[0])\n",
    "    \n",
    "    centroid_x = [ x[0] for x in centroid_list]\n",
    "    centroid_y = [ y[1] for y in centroid_list]\n",
    "\n",
    "    data_centroid_x = np.append(data_centroid_x, (centroid_x))\n",
    "    data_centroid_y = np.append(data_centroid_y, (centroid_y))\n",
    "    \n",
    "    data_centroid_list_x = np.append(data_centroid_list_x, np.mean(centroid_x))\n",
    "    data_centroid_list_y = np.append(data_centroid_list_y, np.mean(centroid_y))\n",
    "          \n",
    "\n",
    "    # Visualize centroids\n",
    "    cv2.circle(frame, (int(data_centroid_list_x[-1]), int(data_centroid_list_y[-1])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # 4. Display the image with the centroids\n",
    "    cv2.imshow('result', frame)\n",
    "       \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken {}s for {} frames\".format(round(end_time - start_time, 2), total_frames))\n",
    "print(\"frames {} per seconds\".format(total_frames/round(end_time - start_time, 2)))\n",
    "\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9037475-1036-46fb-ab03-b5b3a0465698",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving  image frames with centroids marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb4b1cd-79da-4115-a0f4-a0b7b48f67d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 6.55s for 106 frames\n",
      "frames 16.18320610687023 per seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import time\n",
    "\t\n",
    "# Load a pretrained YOLO11n model\n",
    "model = YOLO(\"./weights/best.pt\")\n",
    "vid_cap = cv2.VideoCapture('./datasets/recording/IMG_5795.MOV')\n",
    "total_frames = 0\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "data_centroid_x = np.empty([0])\n",
    "data_centroid_y = np.empty([0])\n",
    "\n",
    "data_centroid_list_x = np.empty([0])\n",
    "data_centroid_list_y = np.empty([0])\n",
    "data_walkways = np.empty([0])\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while (vid_cap.isOpened()):\n",
    "   \n",
    "    # Read a frame from the video\n",
    "    ret, frame = vid_cap.read()\n",
    "\n",
    "    # If ret is False, it means the video has ended\n",
    "    if not ret:\n",
    "        print(\"Video has finished.\")\n",
    "        break\n",
    "        \n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    inference = model(frame, verbose=False)[0]\n",
    "   \n",
    "    walkway = sv.Detections.from_ultralytics( inference)\n",
    "    #walkway = detections[(detections.class_id == 4) & (detections.confidence > 0.5)] #walkway\n",
    "    \n",
    "    total_frames += 1\n",
    "    \n",
    "    if len(walkway) < 3:\n",
    "        continue      \n",
    "    \n",
    "    data_walkways = np.append(data_walkways, len(walkway))\n",
    "    \n",
    "    centroid_list = [((b1[0] + b1[2])/2, (b1[1] + b1[3])/2) for b1 in walkway.xyxy ]\n",
    "    centroid_list.sort(key=lambda p: p[0])\n",
    "    \n",
    "    centroid_x = [ x[0] for x in centroid_list]\n",
    "    centroid_y = [ y[1] for y in centroid_list]\n",
    "\n",
    "    data_centroid_x = np.append(data_centroid_x, (centroid_x))\n",
    "    data_centroid_y = np.append(data_centroid_y, (centroid_y))\n",
    "    \n",
    "    data_centroid_list_x = np.append(data_centroid_list_x, np.mean(centroid_x))\n",
    "    data_centroid_list_y = np.append(data_centroid_list_y, np.mean(centroid_y))\n",
    "          \n",
    "    # Visualize centroids\n",
    "    cv2.circle(frame, (int(data_centroid_list_x[-1]), int(data_centroid_list_y[-1])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # 4. Display the image with the centroids\n",
    "    cv2.imshow('result', frame)\n",
    "    img_name = './datasets/centroid_images/img_' + str(img_counter) + '_' + str(len(walkway)) +'_' +  str(round(time.perf_counter(), 2)) + '.jpg'\n",
    "    cv2.imwrite(img_name, frame)\n",
    "    \n",
    "    img_counter += 1\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken {}s for {} frames\".format(round(end_time - start_time, 2), total_frames))\n",
    "print(\"frames {} per seconds\".format(total_frames/round(end_time - start_time, 2)))\n",
    "\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f5230-51d7-4a92-bed3-5243d1b18f5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Saving images with walkway detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a753c9-f179-4cef-a04d-32c9fee1bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import time\n",
    "\t\n",
    "# Load a pretrained YOLO11n model\n",
    "model = YOLO(\"./weights/best.pt\")\n",
    "vid_cap = cv2.VideoCapture('./datasets/recording/IMG_5795.MOV')\n",
    "img_counter = 0\n",
    "prev_confidence = 0.0\n",
    "while (vid_cap.isOpened()):\n",
    "   \n",
    "    # Read a frame from the video\n",
    "    ret, frame = vid_cap.read()\n",
    "\n",
    "    # If ret is False, it means the video has ended\n",
    "    if not ret:\n",
    "        print(\"Video has finished.\")\n",
    "        break\n",
    "        \n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    inference = model(frame, verbose=False)[0]\n",
    "    \n",
    "    detections = sv.Detections.from_ultralytics(inference)\n",
    "    \"\"\"\n",
    "    curr_confidence = np.max(detections.confidence)\n",
    "    if( curr_confidence > prev_confidence):\n",
    "        prev_confidence = curr_confidence\n",
    "        walkway = detections[detections.confidence == curr_confidence]\n",
    "    else:\n",
    "        walkway = detections[detections.confidence == prev_confidence]\n",
    "    if( len(walkway) == 0):\n",
    "        continue\n",
    "    \"\"\"\n",
    "    walkway = detections[detections.confidence == np.max(detections.confidence)]\n",
    "   \n",
    "    # Draw the rectangle with highest confidence detection\n",
    "    # Define coordinates for the rectangle -  the coordinates for the top-left and bottom-right corners\n",
    "    x1, y1, x2, y2 = walkway.xyxy[0].astype(int) \n",
    "    bottom_right = (x2, y2)\n",
    "    top_left = (x1, y1)\n",
    "    \n",
    "    # Define color (Blue in BGR) and thickness\n",
    "    color = (255, 0, 0)\n",
    "    thickness = 2  # Fill the rectangle\n",
    "\n",
    "    # Draw the rectangle on the image\n",
    "    cv2.rectangle(frame, top_left, bottom_right, color, thickness)\n",
    "    \n",
    "    # Display the image with the rectangle\n",
    "    cv2.imshow('Image with Rectangle', frame)\n",
    "    img_name = './datasets/walkway_images/path_img_' + str(img_counter) + '_' + str(np.max(detections.confidence)) + '.jpg'\n",
    "    cv2.imwrite(img_name, frame)\n",
    "  \n",
    "    img_counter += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
