{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01fbb65c-094a-421a-8892-daca3b751c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import time\n",
    "\t\n",
    "# Load a pretrained  model\n",
    "#model = YOLO(\"../Vision/weights/best.pt\")\n",
    "\n",
    "# Export the model to NCNN format to be used on Pi\n",
    "#model.export(format=\"ncnn\")  # creates 'yolo11n_ncnn_model'\n",
    "\n",
    "#Load the model\n",
    "model = YOLO(\"../weights/best_ncnn_model\", task='detect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf615d2-f5c9-4ec6-85c0-f0aff93e1023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../weights/best_ncnn_model for NCNN inference...\n",
      "\n",
      "image 1/1 C:\\Aanya\\vision\\Vision\\notebooks\\..\\datasets\\10_03_25_57imgs\\test\\images\\IMG_0432_JPG.rf.051c3c92a77f62b6609178e5f7d85fbb.jpg: 640x640 4 walkways, 71.4ms\n",
      "Speed: 3.5ms preprocess, 71.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Detections(xyxy=array([[     41.492,      500.49,      532.93,      608.93],\n",
       "       [     97.193,      431.14,      490.96,       536.7],\n",
       "       [     112.42,      405.88,      484.24,      521.42],\n",
       "       [     174.11,      295.98,      393.08,      403.04]], dtype=float32), mask=None, confidence=array([    0.72805,      0.3013,     0.28426,     0.27236], dtype=float32), class_id=array([4, 4, 4, 4]), tracker_id=None, data={'class_name': array(['walkway', 'walkway', 'walkway', 'walkway'], dtype='<U7')}, metadata={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the exported NCNN model\n",
    "model = YOLO(\"../weights/best_ncnn_model\", task='detect')\n",
    "\n",
    "# Run inference\n",
    "inference = model(\"../datasets/10_03_25_57imgs/test/images/IMG_0432_JPG.rf.051c3c92a77f62b6609178e5f7d85fbb.jpg\")\n",
    "result = inference[0]\n",
    "detections = sv.Detections.from_ultralytics (result)\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f758e50-9dce-4e14-be4b-53fc06f18993",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591705ab-14e4-42a9-9de1-0ee4e0c56760",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_cap = cv2.VideoCapture('../datasets/recording/IMG_1206.MOV')\n",
    "while (vid_cap.isOpened()):\n",
    "    _, frame = vid_cap.read()\n",
    "    inference = model( frame, verbose=False)[0]\n",
    "\n",
    "    im_bgr = inference.plot()  # BGR-order numpy array\n",
    "    #im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "\n",
    "    cv2.imshow('result', cv2.resize(im_bgr, (640, 640)) )\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "129677d4-edfe-4ffa-855b-b88c355681c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "radius = 5                    # Radius in pixels\n",
    "color = (255, 0, 255)             # BGR color (Green in this case)\n",
    "thickness = 2                   # Thickness of the circle line (-1 for a filled circle)\n",
    "thickness = -1 # -1 indicates a filled circle   \n",
    "\n",
    "\n",
    "vid_cap = cv2.VideoCapture('../datasets/recording/IMG_1206.MOV')\n",
    "while (vid_cap.isOpened()):\n",
    "    _, frame = vid_cap.read()\n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    inference = model(frame, verbose=False)[0]\n",
    "   \n",
    "    detections = sv.Detections.from_ultralytics( inference)\n",
    "    walkway = detections[detections.class_id == 4] #walkway\n",
    "    if len(walkway) < 3:\n",
    "        continue\n",
    "    \n",
    "    centroid_list = [((b1[0] + b1[2])/2, (b1[1] + b1[3])/2) for b1 in walkway.xyxy ]\n",
    "    centroid_x = [ int(x[0]) for x in centroid_list]\n",
    "    centroid_y = [ int(y[1]) for y in centroid_list]\n",
    "    \n",
    "    for i, x in enumerate(centroid_x):\n",
    "        #Define circle parameters\n",
    "        center_coordinates = (x, centroid_y[i])  # (x, y) - center of the circle\n",
    "        # 3. Draw the circle on the image\n",
    "        cv2.circle(frame, center_coordinates, radius, color, thickness)\n",
    "    \n",
    "    # 4. Display the image with the centroids\n",
    "    cv2.imshow('result', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "vid_cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bc7963e-4496-46a2-8126-abf29ad2fb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading ../weights/best_ncnn_model/ for NCNN inference...\n",
      "Time taken 6.51s for 76 frames\n",
      "frames 11.674347158218126 per seconds\n"
     ]
    }
   ],
   "source": [
    "data_centroid_x = np.empty([0])\n",
    "data_centroid_y = np.empty([0])\n",
    "\n",
    "data_centroid_list_x = np.empty([0])\n",
    "data_centroid_list_y = np.empty([0])\n",
    "data_walkways = np.empty([0])\n",
    "\n",
    "model = YOLO(\"../weights/best_ncnn_model/\")\n",
    "vid_cap = cv2.VideoCapture('../datasets/recording/IMG_1206.MOV')\n",
    "total_frames = 0\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "while (vid_cap.isOpened()):\n",
    "   \n",
    "    # Read a frame from the video\n",
    "    ret, frame = vid_cap.read()\n",
    "\n",
    "    # If ret is False, it means the video has ended\n",
    "    if not ret:\n",
    "        print(\"Video has finished.\")\n",
    "        break\n",
    "        \n",
    "    frame = cv2.resize(frame, (640, 640))\n",
    "    inference = model(frame, verbose=False)[0]\n",
    "   \n",
    "    walkway = sv.Detections.from_ultralytics( inference)\n",
    "    #walkway = detections[(detections.class_id == 4) & (detections.confidence > 0.5)] #walkway\n",
    "    \n",
    "    total_frames += 1\n",
    "    \n",
    "    if len(walkway) < 3:\n",
    "        continue      \n",
    "    \n",
    "    data_walkways = np.append(data_walkways, len(walkway))\n",
    "    \n",
    "    centroid_list = [((b1[0] + b1[2])/2, (b1[1] + b1[3])/2) for b1 in walkway.xyxy ]\n",
    "    centroid_list.sort(key=lambda p: p[0])\n",
    "    \n",
    "    centroid_x = [ x[0] for x in centroid_list]\n",
    "    centroid_y = [ y[1] for y in centroid_list]\n",
    "\n",
    "    data_centroid_x = np.append(data_centroid_x, (centroid_x))\n",
    "    data_centroid_y = np.append(data_centroid_y, (centroid_y))\n",
    "    \n",
    "    data_centroid_list_x = np.append(data_centroid_list_x, np.mean(centroid_x))\n",
    "    data_centroid_list_y = np.append(data_centroid_list_y, np.mean(centroid_y))\n",
    "          \n",
    "\n",
    "    # Visualize centroids\n",
    "    cv2.circle(frame, (int(data_centroid_list_x[-1]), int(data_centroid_list_y[-1])), 5, (0, 0, 255), -1)\n",
    "\n",
    "    # 4. Display the image with the centroids\n",
    "    cv2.imshow('result', frame)\n",
    "       \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken {}s for {} frames\".format(round(end_time - start_time, 2), total_frames))\n",
    "print(\"frames {} per seconds\".format(total_frames/round(end_time - start_time, 2)))\n",
    "\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafebbbb-510a-48cf-975f-a6c87ef964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_cap = cv2.VideoCapture(0)\n",
    "while (vid_cap.isOpened()):\n",
    "    _, frame = vid_cap.read()\n",
    "   \n",
    "    cv2.imshow('frame', cv2.resize(frame, (640, 640)) )\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958dc535-4239-4f63-be51-167f936ef864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
